[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Theo Brown",
    "section": "",
    "text": "We say that we will put the sun in a box. The idea is pretty. The problem is, we don‚Äôt know how to make the box.\n‚Äì Pierre Gilles de Gennes\n\n\n\nI‚Äôm a first year PhD student at the University College London, supervised by Ilija Bogunovic.\nMy project is jointly supported by the UK Atomic Energy Authority (UKAEA), a government research institute seeking to lead the delivery of sustainable fusion energy, where I am a visiting researcher.\nI work closely with the UKAEA integrated modelling group, designing scenarios for the UK‚Äôs proposed next-generation fusion reactor."
  },
  {
    "objectID": "index.html#research",
    "href": "index.html#research",
    "title": "Theo Brown",
    "section": "Research",
    "text": "Research\nFusion power could be the biggest game-changer in history; to achieve it, we need advanced control systems and highly optimised designs.\nMy research connects ideas from control, optimisation, Bayesian statistics, and plasma physics in order to tackle the hardest engineering challenge humanity has ever faced.\nBuzzwords that get me excited include:\n\nProbabilistic machine learning\nVerifiably robust and safe algorithms for critical systems\nTrajectory planning and controller design\nAdaptive and learning-based control\nPhysics-informed machine learning\nBayesian optimisation"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Theo Brown",
    "section": "Publications",
    "text": "Publications\n\n\n    \n        \n            \n                \n            \n            \n                \n                    Sample-efficient Bayesian optimisation using known invariances\n                \n                Theodore Brown, Alexandru Cioba & Ilija Bogunovic\n                \n                NeurIPS 2024 \n                \n                \n                    paper\n                \n                \n                    code\n                \n                \n                    \n                        blog\n                    \n                \n                \n                    \n                        slides\n                    \n                \n                \n                    bibtex\n                \n            \n        \n    \n\n    \n        \n            \n                \n            \n            \n                \n                    Multi-objective Bayesian optimization for design of Pareto-optimal current drive profiles in STEP\n                \n                Theodore Brown, Stephen Marsden, Vignesh Gopakumar, Alexander Terenin, Hong Ge & Francis Casson\n                \n                IEEE Transactions on Plasma Science 2024 \n                \n                \n                    paper\n                \n                \n                    code\n                \n                \n                \n                \n                    bibtex\n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Theo Brown",
    "section": "Blog",
    "text": "Blog\n\n\n\n\n\n\n\n\n\n\nHow can we exploit known symmetry in Bayesian optimisation?\n\n\n\n\n\nGeneralised regret bounds for group-invariant BO\n\n\n\n\n\nDec 9, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Theo Brown",
    "section": "Education",
    "text": "Education\nPhD, Machine Learning  University College London | Sep 2023 - present\nMEng, Information and Control Engineering  University of Cambridge | Sep 2016 - Jul 2023  Thesis: Reinforcement learning and Bayesian optimisation for tokamak plasma control"
  },
  {
    "objectID": "index.html#hobbies",
    "href": "index.html#hobbies",
    "title": "Theo Brown",
    "section": "Hobbies",
    "text": "Hobbies\nI‚Äôm often found in the garage doing woodwork or outside looking after my garden. I also love hiking, jazz, classic novels, korfball, and board games.\nI‚Äôm a passionate advocate for wider recognition and research concerning the long-term impacts of mild traumatic brain injury."
  },
  {
    "objectID": "talks/invariantbo/slides.html#bayesian-optimisation",
    "href": "talks/invariantbo/slides.html#bayesian-optimisation",
    "title": "Sample-efficient Bayesian optimisation using known invariances",
    "section": "Bayesian optimisation",
    "text": "Bayesian optimisation\nWide range of applications\n\n\n\n\n\n\n\n\n\nDrug discovery [C4XD]\n\n\n\n\n\n\n\nChip design [NVIDIA]\n\n\n\n\n\n\nVideo\nNuclear fusion reactors[Proxima Fusion]\n\n\n\n\n\nGoal: sample efficiency"
  },
  {
    "objectID": "talks/invariantbo/slides.html#symmetry-and-invariance",
    "href": "talks/invariantbo/slides.html#symmetry-and-invariance",
    "title": "Sample-efficient Bayesian optimisation using known invariances",
    "section": "Symmetry and invariance",
    "text": "Symmetry and invariance\nHow can we exploit symmetry in BO?\n\n\nObjective function is known to be symmetric\nKey insight: making one observation gives additional information\nIn the noiseless case, this is perfect information"
  },
  {
    "objectID": "talks/invariantbo/slides.html#invariant-gaussian-processes",
    "href": "talks/invariantbo/slides.html#invariant-gaussian-processes",
    "title": "Sample-efficient Bayesian optimisation using known invariances",
    "section": "Invariant Gaussian processes",
    "text": "Invariant Gaussian processes\nNaive method: data augmentation\n\n\nKey insight: making one observation gives additional information\n\n\n\n\nData augmentation: add transformed data to dataset \\[\n\\mathcal{D} \\gets \\mathcal{D} \\cup \\{(\\sigma(x), f(x)) \\quad \\forall \\sigma \\in G, x \\in \\mathcal{D}\\}\n\\]\n\n\n\n\nProblem: computational cost of GP scales with \\(\\mathcal{O}(\\textcolor{#9a2515}{|G|^3} n^3)\\)\n\n\nCan we do better?"
  },
  {
    "objectID": "talks/invariantbo/slides.html#invariant-gaussian-processes-1",
    "href": "talks/invariantbo/slides.html#invariant-gaussian-processes-1",
    "title": "Sample-efficient Bayesian optimisation using known invariances",
    "section": "Invariant Gaussian processes",
    "text": "Invariant Gaussian processes\nOur method: invariant kernel\n\n\nConstruct an invariant kernel: \\[\nk_G(x, x') = \\frac{1}{|G|} \\sum_{\\sigma \\in G} k(x, \\sigma(x'))\n\\]\n\n\n\n\nGPs with this kernel are distributions over invariant functions!\n\n\n\n\n\n\n\n\n\n\n\n\nCompute cost reduced from \\(\\mathcal{O}(\\textcolor{#9a2515}{|G|^3} n^3)\\) to \\(\\mathcal{O}(\\textcolor{#259a15}{|G|} n^3)\\)"
  },
  {
    "objectID": "talks/invariantbo/slides.html#sample-complexity-for-invariant-kernel-bo",
    "href": "talks/invariantbo/slides.html#sample-complexity-for-invariant-kernel-bo",
    "title": "Sample-efficient Bayesian optimisation using known invariances",
    "section": "Sample complexity for invariant kernel BO",
    "text": "Sample complexity for invariant kernel BO\nNumber of samples \\(T\\) for precision \\(\\epsilon\\)\n\n\nOur upper bound:\n\n\n\\[\\begin{align}\n        T = \\tilde{\\mathcal{O}}\\left(\n\\left(\n    {\\textcolor{#259a15}{\\frac{1}{|G|}}}\n\\right)^\\frac{2\\nu + d -1}{2 \\nu}\n\\epsilon^{-\\frac{2\\nu + d -1}{\\nu}}\n\\right)\n\\end{align}\\]\n\n\n\n\nLarge \\(|G|\\) ‚Üí large reduction in number of samples\nLower bound in our paper"
  },
  {
    "objectID": "talks/invariantbo/slides.html#synthetic-experiments",
    "href": "talks/invariantbo/slides.html#synthetic-experiments",
    "title": "Sample-efficient Bayesian optimisation using known invariances",
    "section": "Synthetic experiments",
    "text": "Synthetic experiments\nInvariant GP-MVR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInvariant beats standard\nInvariant beats constrained\nUse subgroups for low-cost approximation (2- and 3- block invariance)"
  },
  {
    "objectID": "talks/invariantbo/slides.html#application-fusion-reactor-design",
    "href": "talks/invariantbo/slides.html#application-fusion-reactor-design",
    "title": "Sample-efficient Bayesian optimisation using known invariances",
    "section": "Application: fusion reactor design",
    "text": "Application: fusion reactor design\nHigh-temperature plasma ‚Üí zero-carbon, low-waste energy\n\n\nTask: find an operating point with high stability\nActuators are permutation invariant\n\n\n\n\n\n\n\n\n\nUsing an invariant kernel achieves better results!"
  },
  {
    "objectID": "talks/invariantbo/slides.html#sample-efficient-bayesian-optimisation-using-known-invariances",
    "href": "talks/invariantbo/slides.html#sample-efficient-bayesian-optimisation-using-known-invariances",
    "title": "Sample-efficient Bayesian optimisation using known invariances",
    "section": "Sample-efficient Bayesian optimisation using known invariances",
    "text": "Sample-efficient Bayesian optimisation using known invariances\nü™ß Check out our poster\nüìù Read the paper on arXiv\nüåê See our blog for more info\n‚úâÔ∏è Reach out to theo.brown@ukaea.uk"
  },
  {
    "objectID": "blog/invariantbo/index.html",
    "href": "blog/invariantbo/index.html",
    "title": "How can we exploit known symmetry in Bayesian optimisation?",
    "section": "",
    "text": "Video\nIncorporating symmetry by using an invariant kernel boosts sample efficiency - but by how much?\n\n\nTaking a fully Bayesian approach to optimisation means incorporating all prior knowledge about our objective function into the optimisation process. Often in the physical sciences we come to the table knowing quite a lot about the shape of the objective, whether that‚Äôs from knowledge of the underlying governing equations, physical principles, or geometry of the problem. Our NeurIPS 2024 paper derives new, general guarantees on performance of Bayesian optimisation algorithms that exploit a problem‚Äôs invariances.\n\nWhat is invariance?\nFor example, in molecular optimisation tasks, we might know that our parameterisation of a molecule is under-specified ‚Äì that is, that we can represent the same molecule in many different ways.\nLet‚Äôs say we‚Äôve got 4 ways of parameterising this molecule, A, B, C and D.\nNow, given that these all represent the same molecule, the objective function evaluated at A, B, C, and D should have the same value.\nThis property is known as invariance. Mathematically, a function \\(f\\) is invariant to some transformation \\(\\sigma\\) if \\[\nf(\\sigma(x)) = f(x),\n\\] or, in words, transforming the input does not change the output.\nNote that this is different to the notion of equivariance, a topic that crops up a bunch in deep learning, flow-matching/diffusion models, etc. Equivariance is the relationship ‚Äútransform input, transform output‚Äù, or \\[\nf(\\sigma(x)) = \\sigma (f(x)).\n\\] We won‚Äôt be considering equivariance in this post (although it is also interesting!).\nIn our paper, we build on a well-established method to define group-invariant kernels by analysing their performance in BO. A group is a mathematical object that defines some sort of symmetry. We can view the group as defining a collection of transformations, such as reflections, rotations, permutations, etc. Here are some examples of group-invariant functions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Crystallographic groups [Adams & Orbanz, 2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†2: Dihedral groups\n\n\n\n\n\n\n\nInvariant kernels\nIf the function we‚Äôre modelling exhibits group symmetry, then each point we evaluate gives us additional information about the objective function‚Äôs behaviour elsewhere in the domain ‚Äì information based on all the transformed locations of our original point.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPermuation group\n\n\n\n\n\n\n\nDihedral group\n\n\n\n\n\n\n\nCyclic group\n\n\n\n\n\n\nFigure¬†3: For an invariant objective function, observing at the red point gives exact `bonus‚Äô observations of the function value at the white points. We want to capture this information in the kernel.\n\n\n\n\n\nKernels measure correlation (or shared information) between points. In general, points that are close in space will be closely correlated. For an invariant function, points whose transformed locations are close in space are closely correlated.\nTo understand this, we fix a point (black), and want to measure its correlation with another point (red). If we don‚Äôt factor in the underlying invariance, we can do this by just evaluating the standard kernel, \\[\nk(x_\\mathrm{black}, x_\\mathrm{red}).\n\\]\nIf instead we do take into account the underlying invariance to a group \\(G\\), we need to capture the fact that the transformed points (white) behave exactly the same as the red point. This means we need an ‚Äòinvariant‚Äô kernel, \\(k_G\\), that satisfies \\[\nk_G(x_\\mathrm{black}, x_\\mathrm{red}) = k_G(x_\\mathrm{black}, x_\\mathrm{white}),\n\\] where \\(x_\\mathrm{white} \\in \\{ \\sigma(x_\\mathrm{red}) : \\sigma \\in G \\}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†4: If we capture the underlying invariance (R), the value of the black point is equally correlated with the value at the red and white points\n\n\n\n\n\nTo make a kernel that satisfies this property, we can sum over the kernel values between the black point, the red point, and all transformed versions of each of them. To make sure our kernel is nicely behaved, we normalise by the number of points, making this an average over the transformations: \\[\nk_G(x, x') = \\frac{1}{|G|^2} \\sum_{\\sigma \\in G} \\sum_{\\sigma' \\in G} k(\\sigma(x), \\sigma'(x')).\n\\] For many common kernels, it turns out we only need to consider the ‚Äòone-sided‚Äô transform,\n\\[\nk_G(x, x') = \\frac{1}{|G|} \\sum_{\\sigma \\in G} k(x, \\sigma(x')).\n\\]\nIn our paper, we show that these invariant kernels define a Reproducing Kernel Hilbert Space (function space) containing only \\(G\\)-invariant functions, and relate the RKHS of \\(k_G\\) to the RKHS of \\(k\\).\nArmed with a way of defining invariant functions via a kernel, we can now exploit invariance in any kernelised machine learning algorithm - such as Gaussian processes and Bayesian optimisation.\n\n\nPerformance guarantees of invariant BO\nWhile the idea of invariant kernels isn‚Äôt new, the novel contribution of our paper is to derive bounds on the sample complexity achieved by BO with these kernels. Sample complexity measures how many observations you need to make, \\(T\\), in order to achieve a given regret, \\(\\epsilon\\).\nAs a broad overview, we begin by looking at how the spectral properties of an invariant kernel relate to those of the standard kernel. We find that the symmetrisation process decreases the number of repeated eigenvalues by at least a factor of \\(|G|\\) ‚Äì the number of transformations in the group ‚Äì effectively ‚Äòfolding‚Äô the eigenspace according to the group symmetry. This follows through to reduce the maximal information gain of the kernel, a parameter that quantifies how fast a kernelised algorithm learns about the target function (you can think of this as lower maximal information gain = new observations give us less surprising information about the function = we‚Äôve learnt more about the function from our past observations).\nPiping this faster maximal information gain through the standard BayesOpt regret analysis gives us an upper bound on the sample complexity achieved by invariant BO algorithms, our key result. For example, for the Matern-\\(\\nu\\) kernel in \\(d\\) dimensions, we have \\[\n\\mathrm{Vanilla: \\quad} T = \\tilde{\\mathcal{O}} \\left(\\epsilon^{-\\frac{2\\nu + d - 1}{2\\nu}}\\right)\n\\] \\[\n\\mathrm{Invariant: \\quad} T = \\tilde{\\mathcal{O}} \\left(\\left(\\frac{1}{|G|}\\right)^\\frac{2\\nu + d - 1}{2\\nu} \\epsilon^{-\\frac{2\\nu + d - 1}{2\\nu}}\\right)\n\\] Compared to ‚Äòvanilla‚Äô BO, we‚Äôve achieved a reduction in sample complexity by \\(\\left(\\frac{1}{|G|}\\right)^\\frac{2\\nu + d - 1}{2\\nu}\\)! For completeness, we also come up with an example that gives a lower bound that‚Äôs pretty close to (but not perfectly matching - for good theoretical reasons!) the upper bound, showing that this analysis is tight.\nWe demonstrate this kind of performance in our experiments, with highlights shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†5: Simple regret performance for GP-MVR on synthetic test functions\n\n\n\nWe can see that:\n\nThe invariant kernel method achieves huge improvements in sample complexity.\nThe improvement increases with increasing dimension and increasing group size.\nUsing an invariant kernel actually outperforms constrained optimisation!\n\nThis third point might seem surprising, but one intuition for why is that invariant kernel allows information to ‚Äòbleed‚Äô across the boundaries of the symmetry ‚Äì which constrained BO can‚Äôt do.\n\n\nNeurIPS 2024\nThere‚Äôs a whole lot more to this work that hasn‚Äôt gone into the blog post - check out our paper or code for more info with empirical studies looking at misspecification, quasi-invariance, low-cost approximations, comparisons with data augmentation, and more.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat if our target function is almost invariant?\n\n\n\n\n\n\n\nWhat if we apply this to a difficult problem from nuclear fusion? [ITER.org]\n\n\n\n\n\n\n\n\n\n\nWe‚Äôve also implemented a general library for group-invariant kernels in GPytorch, so you can use an invariant kernel as a drop-in replacement for your Matern-5/2 in BoTorch or your favourite GP/BO library. Have a look at it on Github and PyPI.\nFor more info, or to chat about anything BO/symmetry related, come and check out our poster at NeurIPS! We‚Äôll be in the West Ballroom A-D #6003 on Friday 13th, 1100-1400.\n‚Äì TB\n back to home"
  }
]